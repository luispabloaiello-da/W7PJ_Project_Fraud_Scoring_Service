{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lib/validate_input.py\n",
    "\n",
    "Project Goal:\n",
    "Build a fraud-scoring service that flags suspicious transactions with high accuracy.\n",
    "To train reliable classifiers, we need clean, complete data:\n",
    "  - All features and the target must be present.\n",
    "  - Date and numeric columns must use correct types.\n",
    "  - No missing values or duplicate records should slip into training.\n",
    "\n",
    "Why validate first?\n",
    "In supervised learning, models learn patterns from labeled examples.\n",
    "If columns are missing or mistyped, downstream steps (cleaning, feature creation,\n",
    "model training) will fail or produce misleading results.\n",
    "Early checks enforce the “contract” between our raw input and the pipeline:\n",
    "  • Schema consistency for reproducible workflows \n",
    "  • Correct dtypes for time-based features and numeric scaling \n",
    "  • Awareness of nulls/duplicates so cleaning can handle them explicitly\n",
    "\n",
    "This module offers three functions:\n",
    "  - validate_schema: confirms required columns exist\n",
    "  - validate_types: flags wrong dtypes in datetime columns\n",
    "  - check_nulls_and_duplicates: reports counts for nulls and duplicates\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# List every column our fraud models expect to find\n",
    "REQUIRED_COLUMNS = [\n",
    "    'Transaction_Amount',\n",
    "    'Transaction_Type',\n",
    "    'Timestamp',\n",
    "    'Location',\n",
    "    'Merchant_Category',\n",
    "    'Risk_Score',\n",
    "    'Fraud_Label'\n",
    "]\n",
    "\n",
    "def validate_schema(df: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Ensure that the raw DataFrame contains all required columns.\n",
    "    Raises an error if any are missing, stopping the pipeline early.\n",
    "    \"\"\"\n",
    "    missing = [col for col in REQUIRED_COLUMNS if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    return True\n",
    "\n",
    "def validate_types(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Check that Timestamp is stored as a datetime type.\n",
    "    Correct dtypes are essential for time-based feature engineering downstream.\n",
    "    Returns a dict of issues found (empty if none).\n",
    "    \"\"\"\n",
    "    issues = {}\n",
    "    if 'Timestamp' in df.columns:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['Timestamp']):\n",
    "            issues['Timestamp'] = 'Expected datetime, got object'\n",
    "    return issues\n",
    "\n",
    "def check_nulls_and_duplicates(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Count missing values in each column and the number of exact duplicate rows.\n",
    "    This prepares us to remove or impute nulls and drop duplicates in the cleaning step.\n",
    "    Returns (null_counts, duplicate_count).\n",
    "    \"\"\"\n",
    "    null_counts = df.isnull().sum()\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    return null_counts, duplicate_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lib/clean_data.py\n",
    "\n",
    "Project Goal:\n",
    "After confirming our raw data has the right columns and types,\n",
    "we need to fill every gap and ensure correct formats before models train.\n",
    "In supervised learning, missing values and wrong types break feature\n",
    "creation and model fitting. This module:\n",
    "\n",
    "  1. Fills numeric gaps with each column’s average.\n",
    "  2. Fills categorical gaps with the most common value.\n",
    "  3. Converts Timestamp text into real datetimes and forward‐fills any missing times.\n",
    "  4. Removes exact duplicate rows.\n",
    "\n",
    "The output is a complete, type-safe DataFrame ready for feature engineering.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean a DataFrame by filling nulls and correcting types:\n",
    "      • Numeric columns → fill with mean  \n",
    "      • Categorical columns → fill with mode  \n",
    "      • Timestamp → convert to datetime, then forward‐fill  \n",
    "      • Drop duplicates  \n",
    "\n",
    "    Returns the cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # 1. Fill numeric columns with their mean\n",
    "    numeric_cols = df.select_dtypes(include='number').columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "    # 2. Fill categorical columns with their most frequent value\n",
    "    categorical_cols = df.select_dtypes(include='object').columns\n",
    "    df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n",
    "\n",
    "    # 3. Fix Timestamp: convert and forward-fill\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "        df['Timestamp'] = df['Timestamp'].ffill()\n",
    "\n",
    "    # 4. Drop any exact duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cbb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lib/feature_engineering.py\n",
    "\n",
    "Project Goal:\n",
    "Turn cleaned transaction data into a set of numeric inputs that our fraud models can learn from.\n",
    "Good features help:\n",
    "  • Distance-based and linear models by scaling numbers\n",
    "  • Tree-based models by encoding categories\n",
    "  • All models by capturing user behavior in derived metrics\n",
    "\n",
    "This module follows three steps:\n",
    "  1. Selection   – drop ID or free-text columns that carry no predictive signal  \n",
    "  2. Transformation – encode categorical fields and standardize numeric ones  \n",
    "  3. Creation    – build new features like total spend or time gaps between transactions  \n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply feature engineering so the output is model-ready:\n",
    "      - Remove unhelpful columns (IDs, free text)\n",
    "      - One-hot encode every text category\n",
    "      - Scale numeric features to mean 0, std 1 for KNN and Logistic Regression\n",
    "      - Create summary features to capture money flow and timing\n",
    "    \"\"\"\n",
    "    # 1. Drop ID or free-text columns\n",
    "    for col in ['Transaction_ID', 'User_ID', 'Free_Text_Description']:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col, axis=1)\n",
    "\n",
    "    # 2. One-hot encode each remaining text column\n",
    "    categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "    if categorical_cols:\n",
    "        df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # 3. Scale numeric features for distance-based and linear models\n",
    "    numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "    # exclude the label column if present\n",
    "    numeric_cols = [c for c in numeric_cols if c.lower() not in ['fraud_label', 'is_fraud']]\n",
    "    if numeric_cols:\n",
    "        scaler = StandardScaler()\n",
    "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # 4. Create derived features\n",
    "    # TotalSpend: sum of all amount-related columns (captures overall spend)\n",
    "    amount_cols = [c for c in df.columns if 'amount' in c.lower()]\n",
    "    if len(amount_cols) > 1:\n",
    "        df['TotalSpend'] = df[amount_cols].sum(axis=1)\n",
    "\n",
    "    # TimeSinceLast: gap in seconds between consecutive transactions (captures user tempo)\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df = df.sort_values(by='Timestamp')\n",
    "        df['TimeSinceLast'] = (\n",
    "            df['Timestamp'].diff().dt.total_seconds().fillna(0)\n",
    "        )\n",
    "        df = df.drop('Timestamp', axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4055b6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     amount  time_gap   amount1   amount2  amount3  merchant_type_grocery  \\\n",
      "0 -0.601093 -0.539164 -0.539164 -0.634337      0.0                   True   \n",
      "1  1.409156  1.401826  1.401826  1.411797      0.0                  False   \n",
      "2 -0.808064 -0.862662 -0.862662 -0.777460      0.0                   True   \n",
      "\n",
      "   location_NY  location_TX  TimeSinceLast  \n",
      "0         True        False            0.0  \n",
      "1        False        False          120.0  \n",
      "2        False         True          180.0  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "test_feature_engineering.py\n",
    "\n",
    "This test covers the data preparation step that:\n",
    "  • Encodes text categories as one-hot flags  \n",
    "  • Scales all numeric inputs to mean 0 and standard deviation 1  \n",
    "  • Builds two new features:\n",
    "      – TotalSpend: sum of all “amount” columns  \n",
    "      – TimeSinceLast: seconds since the previous transaction  \n",
    "  • Drops the original Timestamp once the interval is computed\n",
    "\n",
    "We feed sample transactions into engineer_features() and inspect the output.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# allow import from src/feature_engineering.py\n",
    "sys.path.append('../')  \n",
    "from src.feature_engineering import engineer_features\n",
    "\n",
    "# Sample input matching our feature rules\n",
    "data = {\n",
    "    'merchant_type':     ['grocery', 'electronics', 'grocery'],\n",
    "    'location':          ['NY', 'CA', 'TX'],\n",
    "    'amount':            [120.5, 560.0, 75.25],\n",
    "    'time_gap':          [30, 120, 15],\n",
    "    'amount1':           [50, 200, 25],\n",
    "    'amount2':           [70.5, 360, 50.25],\n",
    "    'amount3':           [0, 0, 0],\n",
    "    'Timestamp': pd.to_datetime([\n",
    "        '2025-10-14 10:00:00',\n",
    "        '2025-10-14 10:02:00',\n",
    "        '2025-10-14 10:05:00'\n",
    "    ])\n",
    "}\n",
    "\n",
    "df_test = pd.DataFrame(data)\n",
    "\n",
    "# Run the feature engineering pipeline\n",
    "df_transformed = engineer_features(df_test)\n",
    "\n",
    "# Inspect resulting columns and first rows\n",
    "print(df_transformed.head())\n",
    "\n",
    "# Expected outcome:\n",
    "# 1) merchant_type → one-hot: merchant_type_grocery  \n",
    "# 2) location → one-hot: location_NY, location_TX  \n",
    "# 3) Numeric columns (amount, time_gap, amount1, amount2, amount3) are scaled  \n",
    "# 4) New TotalSpend column = sum of scaled amount* columns  \n",
    "# 5) New TimeSinceLast column = [0.0, 120.0, 180.0] (seconds)  \n",
    "# 6) Original Timestamp is dropped  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
