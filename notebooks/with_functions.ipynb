{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52483679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fraud_models.py\n",
    "import yaml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# Point to the project root (adjust parents[1] to parents[2] if your notebook is deeper)\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# <-- Imports custom preprocessing functions from 'functions.py' <-- #\n",
    "\n",
    "# from lib.functions import ()\n",
    "\n",
    "\n",
    "\n",
    "# <-- Loads YAML configuration to dynamically reference CSV output files. <-- #\n",
    "\n",
    "config = None  # <-- Initialize config\n",
    "try:\n",
    "    with open(\"../config.yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except:\n",
    "    print(\"Yaml configuration file not found!\")\n",
    "    \n",
    "df_fraud_dataset = pd.read_csv(config['input_data']['file1']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8454a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "TARGET_COL = \"Fraud_Label\"\n",
    "TIMESTAMP_COL = \"Timestamp\"  # if present, weâ€™ll extract hour/dow\n",
    "ARTIFACT_DIR = Path(\"./artifacts_proto\")  # output folder\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR = ARTIFACT_DIR / \"models\"\n",
    "PLOTS_DIR = ARTIFACT_DIR / \"plots\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Max rows to train on (stratified) to keep runs fast.\n",
    "MAX_ROWS = 3000   # adjust or set to None for full dataset\n",
    "\n",
    "# A compact but high-signal feature subset (falls back to intersection with your columns)\n",
    "KEEP_COLS = [\n",
    "    # numeric\n",
    "    \"Transaction_Amount\", \"Account_Balance\", \"IP_Address_Flag\",\n",
    "    \"Previous_Fraudulent_Activity\", \"Daily_Transaction_Count\",\n",
    "    \"Avg_Transaction_Amount_7d\", \"Failed_Transaction_Count_7d\",\n",
    "    \"Card_Age\", \"Transaction_Distance\", \"Risk_Score\", \"Is_Weekend\",\n",
    "    # categorical (typical low/med cardinality)\n",
    "    \"Transaction_Type\", \"Device_Type\", \"Card_Type\", \"Authentication_Method\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14f75a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def safe_onehot(*, handle_unknown=\"ignore\", prefer_dense=True):\n",
    "    \"\"\"\n",
    "    Return a OneHotEncoder that works across scikit-learn versions.\n",
    "    - Newer sklearn: OneHotEncoder(sparse_output=False)\n",
    "    - Older sklearn: OneHotEncoder(sparse=False)\n",
    "    \"\"\"\n",
    "    if prefer_dense:\n",
    "        try:\n",
    "            return OneHotEncoder(handle_unknown=handle_unknown, sparse_output=False)\n",
    "        except TypeError:\n",
    "            return OneHotEncoder(handle_unknown=handle_unknown, sparse=False)\n",
    "    return OneHotEncoder(handle_unknown=handle_unknown)\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_proba):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_proba),\n",
    "        \"pr_auc\": average_precision_score(y_true, y_proba),\n",
    "        \"n_test\": int(len(y_true)),\n",
    "        \"positive_rate_test\": float(np.mean(y_true)),\n",
    "    }\n",
    "\n",
    "def plot_and_save_cm(cm, title, out_path):\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    for (i, j), z in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(z), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_and_save_curve(x, y, xlab, ylab, title, out_path):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, y)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af234b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_13712\\2272220290.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(frac=frac, random_state=42)))[\"idx\"].values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved to: D:\\vscode101\\W7PJ_Project_Fraud_Detection_Transactions\\notebooks\\artifacts_proto\n",
      "                model  accuracy  precision    recall        f1   roc_auc  \\\n",
      "0        RandomForest  1.000000      1.000  1.000000  1.000000  1.000000   \n",
      "1  LogisticRegression  0.781667      0.625  0.803109  0.702948  0.885348   \n",
      "\n",
      "     pr_auc  n_test  positive_rate_test  \n",
      "0  1.000000     600            0.321667  \n",
      "1  0.793421     600            0.321667  \n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "assert TARGET_COL in df_fraud_dataset.columns, f\"{TARGET_COL} not found.\"\n",
    "\n",
    "# Keep only available columns from KEEP_COLS\n",
    "feature_cols = [c for c in KEEP_COLS if c in df_fraud_dataset.columns]\n",
    "X = df_fraud_dataset[feature_cols].copy()\n",
    "y = df_fraud_dataset[TARGET_COL].astype(int)\n",
    "\n",
    "# Optional: extract time features if Timestamp exists\n",
    "if TIMESTAMP_COL in df_fraud_dataset.columns:\n",
    "    ts = pd.to_datetime(df_fraud_dataset[TIMESTAMP_COL], errors=\"coerce\")\n",
    "    X[\"tx_hour\"] = ts.dt.hour\n",
    "    X[\"tx_dow\"] = ts.dt.dayofweek\n",
    "    if \"tx_hour\" not in feature_cols: feature_cols.append(\"tx_hour\")\n",
    "    if \"tx_dow\" not in feature_cols: feature_cols.append(\"tx_dow\")\n",
    "\n",
    "# Stratified sample for speed\n",
    "if MAX_ROWS and len(df_fraud_dataset) > MAX_ROWS:\n",
    "    frac = MAX_ROWS / len(df_fraud_dataset)\n",
    "    idx = (df_fraud_dataset[[TARGET_COL]].assign(idx=np.arange(len(df_fraud_dataset)))\n",
    "           .groupby(TARGET_COL, group_keys=False)\n",
    "           .apply(lambda x: x.sample(frac=frac, random_state=42)))[\"idx\"].values\n",
    "    X = X.iloc[idx].reset_index(drop=True)\n",
    "    y = y.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "# Train/test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Identify numeric/categorical columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", safe_onehot())\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Models to train\n",
    "# -----------------------\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=300, class_weight=\"balanced\"),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, random_state=42, n_jobs=-1, class_weight=\"balanced\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "results = []\n",
    "pred_cache = {}\n",
    "\n",
    "for name, estimator in models.items():\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", estimator)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    if hasattr(pipe.named_steps[\"model\"], \"predict_proba\"):\n",
    "        y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # rarely used here, but just in case\n",
    "        y_proba = y_pred.astype(float)\n",
    "\n",
    "    # Metrics\n",
    "    m = {\"model\": name}\n",
    "    m.update(compute_metrics(y_test, y_pred, y_proba))\n",
    "    results.append(m)\n",
    "\n",
    "    # Persist model\n",
    "    joblib.dump(pipe, MODELS_DIR / f\"{name}.pkl\")\n",
    "\n",
    "    # Cache predictions/probas for best-model artifacts later\n",
    "    pred_cache[name] = (y_pred, y_proba)\n",
    "\n",
    "# Save comparison table\n",
    "cmp_df = pd.DataFrame(results).sort_values(\n",
    "    by=[\"pr_auc\", \"roc_auc\", \"f1\"], ascending=False\n",
    ").reset_index(drop=True)\n",
    "cmp_df.to_csv(ARTIFACT_DIR / \"model_comparison.csv\", index=False)\n",
    "\n",
    "# Choose best model by PR-AUC then ROC-AUC then F1\n",
    "best_model = cmp_df.iloc[0][\"model\"]\n",
    "y_pred_best, y_proba_best = pred_cache[best_model]\n",
    "\n",
    "# Save test scores CSV\n",
    "scored = X_test.copy()\n",
    "scored[\"Fraud_Label_true\"] = y_test.values\n",
    "scored[\"score_proba\"] = y_proba_best\n",
    "scored[\"pred_label\"] = y_pred_best\n",
    "scored.to_csv(ARTIFACT_DIR / f\"test_scores_{best_model}.csv\", index=False)\n",
    "\n",
    "# Confusion matrix & curves for best model\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plot_and_save_cm(\n",
    "    cm, f\"{best_model} - Confusion Matrix\",\n",
    "    PLOTS_DIR / f\"confusion_matrix_{best_model}.png\"\n",
    ")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test.values, y_proba_best)\n",
    "plot_and_save_curve(\n",
    "    fpr, tpr, \"False Positive Rate\", \"True Positive Rate\",\n",
    "    f\"ROC Curve - {best_model}\", PLOTS_DIR / f\"roc_{best_model}.png\"\n",
    ")\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test.values, y_proba_best)\n",
    "plot_and_save_curve(\n",
    "    rec, prec, \"Recall\", \"Precision\",\n",
    "    f\"Precision-Recall Curve - {best_model}\", PLOTS_DIR / f\"pr_{best_model}.png\"\n",
    ")\n",
    "\n",
    "print(\"Artifacts saved to:\", ARTIFACT_DIR.resolve())\n",
    "print(cmp_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "W7PJ_Project_Fraud_Detection_Transactions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
